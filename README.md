# README

# Packages used:

1. **LANE DETECTION**: 
		*MODEL*: mask RCNN (https://thesai.org/Downloads/Volume14No5/Paper_58-Mask_R_CNN_Approach_to_Real_Time_Lane_Detection.pdf):
		*DaATASET*: https://www.kaggle.com/datasets/sovitrath/road-lane-instance-segmentation)
	
	The Mask R-CNN model is trained on the above dataset and the model producing bounding boxs on the solid line, dotted line and the divders. We then fit a bazier's curve equation and extract points which fit the line and use that to render on blender. 
	However, this model is tenative as the efficiency is not good. We might be changing the dataset or might go better models like YOLOP v2.
	
2. **CARS, PEDESTRIANS, STOP SIGNS, TRAFFIC SIGNALS**: 
		*MODEL* : YOLOv9 (https://github.com/ultralytics/ultralytics)
		*DATASET*: MSCOCO (https://cocodataset.org/#home)
	
	The Object detection is handled by the widely used Yolo model of the latest v9 version and is trained with the MSCOCO Dataset. The dataset identifies cars, pedestrians, stop signs and traffic signals with ease at a very good accuracy. We will be moving on with this model and wont be changing it for the upcoming phases.
	
	
3. **HUMAN ORIENTATION** :
		*MODEL* : I2L-MeshNet(https://github.com/mks0601/I2L-MeshNet_RELEASE)
		*DATASET*: HUMAN3.6M, MSCOCO, MuCO

	The I2L MeshNet is trained on the MSCOCO, MuCo and Human3.6M datasets. The model input is a image frame and the bounding box of the human in pixel coordinates(extracted from yolov9) and it outputs the 3D pose of the human and forms a mesh of the human. The output is in the format of .obj file which can extracted into the blender environment and rendered accordingly. 
	
4. **OBJECT ORIENTATION**:
		*MODEL* : YOLO3D (https://github.com/ruhyadi/YOLO3D)
		*DATASET* : MSCOCO, KITTI 
		
	The Yolo3D model uses the yolov5 to detect the objects in the image and returns the 2D bounding box of the detected objects. The model then uses a regressor network to generate the 3D bounding boxes for the identified objects and returns the 3D coordinates along with its orientation(yaw). The angle is then used in blender to generate the oreintation of the objects in the enironment.
	
5. **BINS,CONES, CLASSIFICATION OF OTHER OBJECTS:**
		*MODEL* : Dietic (https://github.com/facebookresearch/Detic)
		*DATASET* : COCO, OpenImages, or Objects36
	
	The Object Detections of dustins, traffic cones and classification of vehicles is handled by the Dietic Dataset. The dataset distinguishes sedan, SUV's, minivans, trucks and buses and identifies cones and dustbins classes among other objects.
	
6. **DEPTH ESTIMATION**: 
		*MODEL*: MariGold (https://github.com/prs-eth/Marigold)
	
	As mentioned in the previous phase of the limitation on scale ambiguity for large distances and limitation on outdoor settings, we used the MARIGOLD diffuser model trained by LLMS to improve accuracy. Marigold provided us with the metric depth of every pixel in each frame.
	We used these depths to scale and place out object models in Blender.

7. **TRAFFIC LIGHT CLASSIFICATION**: 
		*MODEL* : Yolov8 (https://github.com/ultralytics/ultralytics)
		*DATASET* : TrafficLight-Detector (https://github.com/Syazvinski/Traffic-Light-Detection-Color-Classification?tab=readme-ov-file#usage)
	
	This repo uses the yolov8 model trained on  a custom dataset allows us to distinguish the colors of the traffic signal. It also produces the arrows of the signals which is to be used in the later phase.
	
8. **TRAFFIC SIGN DETECTION**:
		*MODEL* : Yolov5 (https://github.com/ultralytics/ultralytics)
		*DATASET* : (https://github.com/Anant-mishra1729/Road-sign-detection?tab=readme-ov-file)
	
	We used this custom pretrained weights on the yolov5 model to detect traffic signs like speed limits, crosswalks and arrow markers. However, the result generated by this model is not satisfying. For Phase 3, we will focus to improve the accuracy of this detection by using larger data as the custom dataset was trained on 535 images.

9. **OPTICAL FLOW**
		*MODEL* : RAFT (https://github.com/princeton-vl/RAFT)
		*DATASET* : KITTI DATASET
		
	We used the RAFT model to estimate the optical flow between 2 images. Once, we obtain the flow image, we project the reproject the 1st image on the 2nd image to look for correspondences and estimate the Fundamental Matrix. Then, we use Sampson distance between the reprojected point.

**VIDEO**

*1st VIDEO:*
	*Objects* : Car, Pedestrians, Stop Sign, Dustbins, Solid Lanes, Divider Lanes.
	
*2nd VIDEO*
	*Objects* : Speed Signs, Traffic Cones, Solid Lane, Sedan , SUV, Truck , Traffic Lights, Dotted Lines, Solid Lines
	
